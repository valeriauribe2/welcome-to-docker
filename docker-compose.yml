
services:
  ollama:
    build:
      context: ./ollama
    container_name: ollama
    #volumes:
    #  - ./ollama:/app
    ports:
      - "8080:80"
    networks:
      - ollama-network
    image: ollama/ollama:latest

  llama3:
    build:
      context: ./llama3
    container_name: llama3
    ports:
      - "8081:80"
    networks:
      - ollama-network

  claude:
    build:
      context: ./claude
    container_name: claude
    ports:
      - "8082:80"
    networks:
      - ollama-network
    image: claude/claude:latest






  #swan:
   # build:
    #  context: ./swan
    #container_name: swan
    #ports:
   #   - "5000:5000"
   # networks:
    


#claude:
  #  build:
  #    context: ./claude
   # container_name: claude
  #  ports:
  #    - "8082:80"
 #   networks:
  #    - ollama-network

#webui2:
 #   build:
  #    context: ./webui
 #   container_name: open-webui
   # networks:
   #   - ollama-network
  #  image: ghcr.io/open-webui/open-webui:main
 #  volumes:
  #  - open-webui:/backend/app
  #  depends_on:
  #    - ollama 
  #  ports:
   #   - "3000:8080"
  #  environment:
  #    - 'OLLAMA_BASE_URL=http://ollama:11434'
  #    - 'WEBUI_SECRET_KEY='

  webui:
    build:
      context: ./webui
    container_name: webui
    ports:
      - "3000:3000"
    networks:
      - ollama-network

  #webui:  # This is for your custom WebUI
  #  build:
  #    context: ./webui
  #  container_name: webui
  #  ports:
  #    - "3000:3000"
  #  networks:
  #    - ollama-network

  #open-webui:  # This is for Open WebUI integration
   # image: ghcr.io/open-webui/open-webui:main
   # container_name: open-webui
   # ports:
    #  - "8080:8080"
   # networks:
   #   - ollama-network

networks:
  ollama-network:
    driver: bridge